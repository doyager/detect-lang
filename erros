
"""
#######################################
                ERRORS
#######################################

0. data download script :

from the given urls , to automate the download of zip contents when online and found out the common pattern, i.e."/archive/master.zip" will be
way a url will completed to download it as zip file , so append it to every url
 https://github.com/nodejs/node  ----- >  https://github.com/nodejs/node/archive/master.zip

 b.     using temp file names to avoid overwritting and confusion , so using count in the iterator

1.

endswith function : to check the extension type
issues with using multiple values , created list out of dictionary and then converted dictionary to tuple


2.
Traceback (most recent call last):
  File "<stdin>", line 11, in <module>
  File "/Users/AG03812/workspace/softwares/installed/anaconda3/anaconda3/lib/python3.6/codecs.py", line 321, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfc in position 224: invalid start byte

with open(os.path.join(subdir, file), 'r') as myfile:
to
with open(os.path.join(subdir, file), 'rb') as myfile:

3. stats :

count : 49234 # total no of files with valid extension in all the urls

values per group :

>>> import collections
>>> ounter=collections.Counter(code_languages)
>>> print(ounter)
Counter({'c': 28289, 'js': 15191, 'java': 3177, 'py': 2402, 'any': 52, 'min': 19, 'test': 18, 'api': 7, 'conf': 3, 'umd': 3, '1': 3, 'window': 3, 'es': 3, 'auto': 3, 'command': 2, 'es6': 2, 'config': 2, 'all': 2, 'tonic_example': 2, 'bundle': 2, '0': 2, 'core': 2, 'install': 1, 'foo': 1, 'eslintrc': 1, 'worker': 1, 'node': 1, 'max': 1, 'browser': 1, 'global': 1, 'parse-path': 1, 'parse-search': 1, '0-keep-alive': 1, 'parse-https': 1, 'parse-post': 1, 'parse-auth': 1, 'parse-basic': 1, 'parse-auth-with-header-in-request': 1, 'parse-only-support-http-https-protocol': 1, 'intervals': 1, 'settimeout': 1, 'tls-write': 1, 'setinterval': 1, 'http': 1, 'fsreq-readfile': 1, 'response': 1, 'writestream': 1, 'tcp': 1, 'pipe': 1, 'signal': 1, 'chain-promise-before-init-hooks': 1, 'timeouts': 1, 'zlib-binding': 1, 'request': 1, 'shutdown': 1, 'readstream': 1, 'pipeconnect': 1, 'statwatcher': 1, 'promise-before-init-hooks': 1, 'socket': 1, 'ycm_extra_conf': 1, '6': 1, 'target': 1, 'all-package-search': 1, 'esearch': 1, 'spec': 1, 'gui': 1})

# mistake
on split , I picked index 2 instead of last index hence the code_languages list was polluted with different types of values

file.split(".")[1] ---> file.split(".")[:]

# after above fix

>>> print("count : "+str(count))
count : 49234
>>>
>>> print("code_languages stats: ")
code_languages stats:
>>> counter=collections.Counter(code_languages)
>>> print(counter)
Counter({'c': 28290, 'js': 15360, 'java': 3177, 'py': 2407})


4. install tensor flow

pip3 install --user --upgrade tensorflow

pip3 install keras


5.

tokenizer.fit_on_texts(code_lines)
TypeError: a bytes-like object is required, not 'dict'

Like it has been already mentioned, you are reading the file in binary mode and then creating a list of bytes. In your following for loop you are
 comparing string to bytes and that is where the code is failing.

Decoding the bytes while adding to the list should work. The changed code should look as follows:

with open(fname, 'rb') as f:
    lines = [x.decode('utf8').strip() for x in f.readlines()]

our code ;   data = myfile.read()  -->


6 . after try and catch


>>> print("count : "+str(count))
count : 49220
>>>
>>> print("code_languages stats: ")
code_languages stats:
>>> counter=collections.Counter(code_languages)
>>> print(counter)
Counter({'c': 28290, 'js': 15359, 'java': 3177, 'py': 2394})
>>>
>>> # encoding to tokenize as text

7. variable values

>>> Y = pd.get_dummies(code_languages)
>>> print ("Languages :" +str(len(Y.columns)))
Languages :4


>>> X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)
>>> print ( X_train.shape)
(39376, 100)
>>> print ( X_test.shape)
(9844, 100)
>>>


>>> print(model.summary())
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_1 (Embedding)      (None, 100, 128)          1280000
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 100, 128)          49280
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 25, 128)           0
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 25, 64)            24640
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 12, 64)            0
_________________________________________________________________
lstm_1 (LSTM)                (None, 64)                33024
_________________________________________________________________
dropout_1 (Dropout)          (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
dense_2 (Dense)              (None, 4)                 260
=================================================================
Total params: 1,391,364
Trainable params: 1,391,364
Non-trainable params: 0
_________________________________________________________________
None



-


batch_size = 32
#tbCallBack =TensorBoard(log_dir='G:/tensorflow/text/LanguageDetector/logs', histogram_freq=0, write_graph=True, write_images=True)
history=model.fit(X, Y, epochs = 400, batch_size=batch_size)

Train the model for 400 epochs in mini-batches of 32 samples. This is 40 iterations over all samples in the x_train and y_train tensors.


batch_size = 512
#tbCallBack =TensorBoard(log_dir='G:/tensorflow/text/LanguageDetector/logs', histogram_freq=0, write_graph=True, write_images=True)
history=model.fit(X, Y, epochs = 50, batch_size=batch_size)



"""


